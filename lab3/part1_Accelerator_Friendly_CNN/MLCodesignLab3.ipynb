{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T-iZMy5i6KN",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2a5ac5-0f7e-4c03-fd3e-fe5ef60a6aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 202kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.76MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 17.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import easydict\n",
        "from torchsummary import summary\n",
        "\n",
        "# argument parser\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"lr\": 0.01,\n",
        "})\n",
        "# Hyper Parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = args.epochs\n",
        "batch_size = args.batch_size\n",
        "learning_rate = args.lr\n",
        "\n",
        "# MNIST Dataset (Images and Labels)\n",
        "train_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "test_set = dsets.FashionMNIST(\n",
        "    root = './data/FashionMNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "# Dataset Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MyConvNet(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)"
      ],
      "metadata": {
        "id": "fUTweryu5rDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (1, 28, 28))"
      ],
      "metadata": {
        "id": "WL2B0QYb-QHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffeb35c-898f-40d0-e288-83f3e4a9f2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
            "            Conv2d-4           [-1, 32, 14, 14]           4,608\n",
            "              ReLU-5           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
            "            Linear-7                   [-1, 10]          15,680\n",
            "================================================================\n",
            "Total params: 20,432\n",
            "Trainable params: 20,432\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.40\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---Training started\")\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        L1norm = model.parameters()\n",
        "        arr = []\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name.split('.'):\n",
        "            arr.append(param)\n",
        "        L1loss = 0\n",
        "        for Losstmp in arr:\n",
        "          L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 600 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "-xxlXQl7jCob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef737989-c49d-4448-dc05-8de96142fc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 1875], Loss: 0.3892\n",
            "Epoch: [ 1/ 10], Step: [ 1200/ 1875], Loss: 0.1307\n",
            "Epoch: [ 1/ 10], Step: [ 1800/ 1875], Loss: 0.6903\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 1875], Loss: 0.3467\n",
            "Epoch: [ 2/ 10], Step: [ 1200/ 1875], Loss: 0.2349\n",
            "Epoch: [ 2/ 10], Step: [ 1800/ 1875], Loss: 0.3979\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 1875], Loss: 0.1799\n",
            "Epoch: [ 3/ 10], Step: [ 1200/ 1875], Loss: 0.3809\n",
            "Epoch: [ 3/ 10], Step: [ 1800/ 1875], Loss: 0.4327\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 1875], Loss: 0.1758\n",
            "Epoch: [ 4/ 10], Step: [ 1200/ 1875], Loss: 0.4870\n",
            "Epoch: [ 4/ 10], Step: [ 1800/ 1875], Loss: 0.2944\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 1875], Loss: 0.2735\n",
            "Epoch: [ 5/ 10], Step: [ 1200/ 1875], Loss: 0.4666\n",
            "Epoch: [ 5/ 10], Step: [ 1800/ 1875], Loss: 0.2832\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 1875], Loss: 0.1872\n",
            "Epoch: [ 6/ 10], Step: [ 1200/ 1875], Loss: 0.1457\n",
            "Epoch: [ 6/ 10], Step: [ 1800/ 1875], Loss: 0.1563\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 1875], Loss: 0.0951\n",
            "Epoch: [ 7/ 10], Step: [ 1200/ 1875], Loss: 0.0831\n",
            "Epoch: [ 7/ 10], Step: [ 1800/ 1875], Loss: 0.1969\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 1875], Loss: 0.1300\n",
            "Epoch: [ 8/ 10], Step: [ 1200/ 1875], Loss: 0.2341\n",
            "Epoch: [ 8/ 10], Step: [ 1800/ 1875], Loss: 0.3362\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 1875], Loss: 0.0608\n",
            "Epoch: [ 9/ 10], Step: [ 1200/ 1875], Loss: 0.2198\n",
            "Epoch: [ 9/ 10], Step: [ 1800/ 1875], Loss: 0.2884\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 1875], Loss: 0.1138\n",
            "Epoch: [ 10/ 10], Step: [ 1200/ 1875], Loss: 0.1622\n",
            "Epoch: [ 10/ 10], Step: [ 1800/ 1875], Loss: 0.1718\n",
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), './saved_myconvnet_lab4.pt')"
      ],
      "metadata": {
        "id": "i6dFkcnKQBpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = MyConvNet(args)\n",
        "load_model.load_state_dict(torch.load('./saved_myconvnet_lab4.pt', weights_only=False))\n",
        "\n",
        "load_model = load_model.cuda()\n",
        "correct = 0\n",
        "total = 0\n",
        "load_model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = load_model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN2ge-5n4yI7",
        "outputId": "c0afb6bb-c8c4-4d3a-e430-aec24220c3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for test images:  90 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Lab 3 Part 1 -\n",
        "### Quantization functions\n",
        "from copy import deepcopy\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "def stochastic_round(x):\n",
        "    x_low = torch.floor(x)\n",
        "    x_high = torch.ceil(x)\n",
        "    frac = x - x_low\n",
        "\n",
        "    return torch.where(torch.rand_like(x) < frac, x_high, x_low)\n",
        "\n",
        "def squant(max_value, num_bits, x):\n",
        "    # Uniform symetric quantizer that\n",
        "    # quantize x into sf * qx\n",
        "    # sf: scaling factor\n",
        "    # qx: integer in range [-2^(bits-1)+1, 2^(bits-1)-1]\n",
        "    # note: only 2^bits - 1 different values can be represented, bits >= 2\n",
        "\n",
        "    sf = max_value / (2**(num_bits-1) - 1)\n",
        "    qx = stochastic_round(x/sf)\n",
        "    qx = torch.clip(qx,min=-2**(num_bits-1) + 1, max=2**(num_bits-1) - 1)\n",
        "    dqx = qx * sf\n",
        "    return dqx\n",
        "\n",
        "\n",
        "def asquant(max_value, min_value, num_bits, x):\n",
        "    # Uniform Asymetric quantizer that\n",
        "    # quantize x into min_value + sf * qx\n",
        "    # sf: scaling factor\n",
        "    # qx: integer in range [0,2^bits-1]\n",
        "    # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "    sf = (max_value - min_value) / (2**num_bits - 1)\n",
        "    qx = stochastic_round((x-min_value)/sf)\n",
        "    qx = torch.clip(qx,min=0,max=2**(num_bits) - 1)\n",
        "    dqx = min_value + qx * sf\n",
        "    return dqx\n",
        "\n",
        "### General Functions\n",
        "\n",
        "def evaluate_model(test_model, model_name):\n",
        "    print(\"-------------------------------------\")\n",
        "    print(f\"Evaluating {model_name}\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = test_model(images)\n",
        "        testloss = criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy on the 10000 test images: % .2f %%' % (100 * correct/ total))\n",
        "    print()\n",
        "    print(\"Size of\", model_name)\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters())}\")\n",
        "    print()\n",
        "\n",
        "    all_non_zeros = 0\n",
        "\n",
        "    print(\"Parameters per layer:\")\n",
        "    for name, param in test_model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            non_zero = param.nonzero().size(0)\n",
        "            all_non_zeros += non_zero\n",
        "            total_params = param.numel()\n",
        "            print(f\"Layer: {name} | Non-zero params: {non_zero} | Total params: {total_params} | Sparsity: {100 * (1 -  non_zero / total_params):.2f}%\")\n",
        "\n",
        "    print(f\"Model has {all_non_zeros} non-zero params in total\")\n",
        "    print()\n",
        "\n",
        "    param_size = 4  # 4 bytes per float32 parameter\n",
        "    model_size_kb = all_non_zeros * param_size / 1024\n",
        "    print(f\"Approximate model size: {model_size_kb:.2f} KB\")\n",
        "    print(\"-------------------------------------\")\n"
      ],
      "metadata": {
        "id": "zKhvb7Jgp7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd = load_model.state_dict() # state dictionary of original model"
      ],
      "metadata": {
        "id": "y9o-ARaob0C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Signed Symmetrical Quantization\n",
        "q_model = deepcopy(load_model)\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "max_val = 0\n",
        "for name,_ in load_model.named_parameters():\n",
        "    if torch.max(torch.abs(sd[name])) > max_val:\n",
        "        max_val = torch.max(torch.abs(sd[name]))\n",
        "\n",
        "# Sweep over different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'quantizing model into {n_bits} bits')\n",
        "    for name,_ in load_model.named_parameters():\n",
        "        # print('quantizing ',name)\n",
        "        q_sd[name] = squant(max_val, n_bits, sd[name])\n",
        "\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    evaluate_model(q_model, f\"Symm Quant Model, n={n_bits}\")"
      ],
      "metadata": {
        "id": "qZwqmwvRVDy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9c5231-de5c-4ed6-b866-2ceb4ed5a307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  82.35 %\n",
            "\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  89.58 %\n",
            "\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  89.63 %\n",
            "\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  89.61 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Asymmetrical Quantization\n",
        "q_model = deepcopy(load_model)\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "max_val = 0\n",
        "min_val = 1000\n",
        "for name,_ in load_model.named_parameters():\n",
        "    if torch.max(sd[name]) > max_val:\n",
        "        max_val = torch.max(sd[name])\n",
        "    if torch.min(sd[name]) < min_val:\n",
        "        min_val = torch.min(sd[name])\n",
        "\n",
        "# Sweep over different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'quantizing model into {n_bits} bits')\n",
        "    for name,_ in load_model.named_parameters():\n",
        "        # print('quantizing ',name)\n",
        "        q_sd[name] = asquant(max_val, min_val, n_bits, sd[name])\n",
        "\n",
        "    q_model.load_state_dict(q_sd)\n",
        "\n",
        "    evaluate_model(q_model, f\"Asymm Quant Model, n={n_bits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx44gNMG1aIE",
        "outputId": "d1d3619c-0c24-41a0-d4cf-f3ac1595a2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  87.18 %\n",
            "\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  89.67 %\n",
            "\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  89.61 %\n",
            "\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  89.61 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### For Signed Symmetrical Quantization - but this time we will change [q_min, q_max]\n",
        "q_model = deepcopy(load_model)\n",
        "q_sd = q_model.state_dict()\n",
        "\n",
        "max_vals = []\n",
        "for name,_ in load_model.named_parameters():\n",
        "    max_vals.append(torch.max(torch.abs(sd[name])))\n",
        "max_vals.sort(reverse=True)\n",
        "\n",
        "# Sweep over different bitwidths\n",
        "print(\"Limiting range for signed symmetrical quantization:\")\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    for limit in range(len(max_vals)): # for taking the nth highest and lowest values in tensor\n",
        "        if limit == 0:\n",
        "            continue\n",
        "\n",
        "        print(f'quantizing model into {n_bits} bits, and limiting range by {limit} values')\n",
        "\n",
        "        for name,_ in load_model.named_parameters():\n",
        "            # print('quantizing ',name)\n",
        "            q_sd[name] = squant(max_vals[limit], n_bits, sd[name])\n",
        "\n",
        "        q_model.load_state_dict(q_sd)\n",
        "\n",
        "        evaluate_model(q_model, f\"Range limited Symm Quant Model, n={n_bits}, lim={limit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKEsuJCVDgPC",
        "outputId": "ab3014da-6660-47e7-b199-53e08587685c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limiting range for signed symmetrical quantization:\n",
            "quantizing model into 4 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.21 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=4, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 132 | Total params: 144 | Sparsity: 8.33%\n",
            "Layer: conv2.weight | Non-zero params: 2884 | Total params: 4608 | Sparsity: 37.41%\n",
            "Layer: lin2.weight | Non-zero params: 8244 | Total params: 15680 | Sparsity: 47.42%\n",
            "Model has 11260 non-zero params in total\n",
            "\n",
            "Approximate model size: 43.98 KB\n",
            "-------------------------------------\n",
            "quantizing model into 4 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.45 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=4, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 135 | Total params: 144 | Sparsity: 6.25%\n",
            "Layer: conv2.weight | Non-zero params: 3036 | Total params: 4608 | Sparsity: 34.11%\n",
            "Layer: lin2.weight | Non-zero params: 8758 | Total params: 15680 | Sparsity: 44.15%\n",
            "Model has 11929 non-zero params in total\n",
            "\n",
            "Approximate model size: 46.60 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.51 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=8, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 143 | Total params: 144 | Sparsity: 0.69%\n",
            "Layer: conv2.weight | Non-zero params: 4501 | Total params: 4608 | Sparsity: 2.32%\n",
            "Layer: lin2.weight | Non-zero params: 15096 | Total params: 15680 | Sparsity: 3.72%\n",
            "Model has 19740 non-zero params in total\n",
            "\n",
            "Approximate model size: 77.11 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.85 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=8, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 143 | Total params: 144 | Sparsity: 0.69%\n",
            "Layer: conv2.weight | Non-zero params: 4510 | Total params: 4608 | Sparsity: 2.13%\n",
            "Layer: lin2.weight | Non-zero params: 15138 | Total params: 15680 | Sparsity: 3.46%\n",
            "Model has 19791 non-zero params in total\n",
            "\n",
            "Approximate model size: 77.31 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.59 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=12, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4603 | Total params: 4608 | Sparsity: 0.11%\n",
            "Layer: lin2.weight | Non-zero params: 15641 | Total params: 15680 | Sparsity: 0.25%\n",
            "Model has 20388 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.64 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.93 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=12, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4605 | Total params: 4608 | Sparsity: 0.07%\n",
            "Layer: lin2.weight | Non-zero params: 15646 | Total params: 15680 | Sparsity: 0.22%\n",
            "Model has 20395 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.67 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.57 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=16, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.93 %\n",
            "-------------------------------------\n",
            "Size of Range limited Symmetrically quantized model bit_width=16, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Same for Asymmetrical Quantization\n",
        "q_model_asymm_2 = deepcopy(load_model)\n",
        "q_asymm_sd_2 = q_model_asymm_2.state_dict()\n",
        "\n",
        "max_vals = []\n",
        "min_vals = []\n",
        "for name,_ in load_model.named_parameters():\n",
        "    max_vals.append(torch.max(sd[name]))\n",
        "    min_vals.append(torch.min(sd[name]))\n",
        "max_vals.sort(reverse=True)\n",
        "min_vals.sort()\n",
        "\n",
        "print(\"Limiting range for asymmetrical quantization:\")\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    for limit in range(len(max_vals)): # for taking the nth highest and lowest values in tensor\n",
        "        if limit == 0:\n",
        "            continue\n",
        "\n",
        "        print(f'quantizing model into {n_bits} bits, and limiting range by {limit} values')\n",
        "\n",
        "        for name,_ in load_model.named_parameters():\n",
        "            # print('quantizing ',name)\n",
        "            q_asymm_sd_2[name] = asquant(max_vals[limit], min_vals[limit], n_bits, sd[name])\n",
        "\n",
        "        q_model_asymm_2.load_state_dict(q_asymm_sd_2)\n",
        "\n",
        "        # Test the Model\n",
        "        evaluate_model(q_model, f\"Range limited Asymm Quant Model, n={n_bits}, lim={limit}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPIFtvkvQy9W",
        "outputId": "0a19c525-2cc3-49b8-d46b-cc0b34bb9ba7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limiting range for asymmetrical quantization:\n",
            "quantizing model into 4 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  86.96 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=4, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 4 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.06 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=4, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.64 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=8, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.93 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=8, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.62 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=12, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.81 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=12, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits, and limiting range by 1 values\n",
            "Accuracy on the 10000 test images:  88.62 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=16, n=1\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits, and limiting range by 2 values\n",
            "Accuracy on the 10000 test images:  87.81 %\n",
            "-------------------------------------\n",
            "Size of Range limited Asymmetrically quantized model bit_width=16, n=2\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### For per-layer Symmetrical Quantization\n",
        "q_model_symm_3 = deepcopy(load_model)\n",
        "q_symm_sd_3 = q_model_symm_3.state_dict()\n",
        "\n",
        "# Sweep over different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'quantizing model into {n_bits} bits')\n",
        "    for name,_ in load_model.named_parameters():\n",
        "        # print('quantizing ',name)\n",
        "        q_symm_sd_3[name] = squant(torch.max(torch.abs(sd[name])), n_bits, sd[name])\n",
        "\n",
        "    q_model_symm_3.load_state_dict(q_symm_sd_3)\n",
        "\n",
        "    # Test the Model\n",
        "    evaluate_model(q_model_symm_3, f\"Symmetrically quantized model, per-layer with bit_width = {n_bits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhMQQswwAWGK",
        "outputId": "f4ccc9dc-2a3e-47cc-aa81-3d7e73c4dfed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  89.15 %\n",
            "-------------------------------------\n",
            "Size of Symmetrically quantized model, per-layer with bit_width = 4\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 111 | Total params: 144 | Sparsity: 22.92%\n",
            "Layer: conv2.weight | Non-zero params: 2928 | Total params: 4608 | Sparsity: 36.46%\n",
            "Layer: lin2.weight | Non-zero params: 8799 | Total params: 15680 | Sparsity: 43.88%\n",
            "Model has 11838 non-zero params in total\n",
            "\n",
            "Approximate model size: 46.24 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "-------------------------------------\n",
            "Size of Symmetrically quantized model, per-layer with bit_width = 8\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 140 | Total params: 144 | Sparsity: 2.78%\n",
            "Layer: conv2.weight | Non-zero params: 4487 | Total params: 4608 | Sparsity: 2.63%\n",
            "Layer: lin2.weight | Non-zero params: 15146 | Total params: 15680 | Sparsity: 3.41%\n",
            "Model has 19773 non-zero params in total\n",
            "\n",
            "Approximate model size: 77.24 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "-------------------------------------\n",
            "Size of Symmetrically quantized model, per-layer with bit_width = 12\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4603 | Total params: 4608 | Sparsity: 0.11%\n",
            "Layer: lin2.weight | Non-zero params: 15651 | Total params: 15680 | Sparsity: 0.18%\n",
            "Model has 20398 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.68 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.18 %\n",
            "-------------------------------------\n",
            "Size of Symmetrically quantized model, per-layer with bit_width = 16\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### For per-layer Asymmetrical Quantization\n",
        "q_model_asymm_3 = deepcopy(load_model)\n",
        "q_asymm_sd_3 = q_model_asymm_3.state_dict()\n",
        "\n",
        "# Sweep over different bitwidths\n",
        "for n_bits in (4, 8, 12, 16):\n",
        "    print(f'quantizing model into {n_bits} bits')\n",
        "    for name,_ in load_model.named_parameters():\n",
        "        # print('quantizing ',name)\n",
        "        q_asymm_sd_3[name] = asquant(torch.max(sd[name]), torch.min(sd[name]), n_bits, sd[name])\n",
        "\n",
        "    q_model_asymm_3.load_state_dict(q_asymm_sd_3)\n",
        "\n",
        "    evaluate_model(q_model_asymm_3, f\"Asymmetrically quantized model, per-layer with bit_width = {n_bits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FnzILNYeYeu",
        "outputId": "85ff18a4-ec2f-4182-d6d8-87c030e5004a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantizing model into 4 bits\n",
            "Accuracy on the 10000 test images:  89.38 %\n",
            "-------------------------------------\n",
            "Size of Asymmetrically quantized model, per-layer with bit_width = 4\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 8 bits\n",
            "Accuracy on the 10000 test images:  90.22 %\n",
            "-------------------------------------\n",
            "Size of Asymmetrically quantized model, per-layer with bit_width = 8\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 12 bits\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "-------------------------------------\n",
            "Size of Asymmetrically quantized model, per-layer with bit_width = 12\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "quantizing model into 16 bits\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "-------------------------------------\n",
            "Size of Asymmetrically quantized model, per-layer with bit_width = 16\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Task 2: Activation + Weight Quantization\n",
        "\n",
        "max_val = 0\n",
        "min_val = 1000\n",
        "for name,_ in model.named_parameters():\n",
        "    if torch.max(sd[name]) > max_val:\n",
        "        max_val = torch.max(sd[name])\n",
        "    if torch.min(sd[name]) < min_val:\n",
        "        min_val = torch.min(sd[name])\n",
        "\n",
        "class MyConvNet2(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = asquant(max_val, min_val, 16, self.conv1(x))\n",
        "        x = asquant(max_val, min_val, 16, self.act1(x))\n",
        "        x = asquant(max_val, min_val, 16, self.pool1(x))\n",
        "        x = asquant(max_val, min_val, 16, self.conv2(x))\n",
        "        x = asquant(max_val, min_val, 16, self.act2(x))\n",
        "        x = asquant(max_val, min_val, 16, self.pool2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MyConvNet2(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)\n",
        "\n",
        "print(\"---Training started for Task 2: Activation + Weight Quantization\")\n",
        "# Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        L1norm = model.parameters()\n",
        "        arr = []\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name.split('.'):\n",
        "            arr.append(param)\n",
        "        L1loss = 0\n",
        "        for Losstmp in arr:\n",
        "          L1loss = L1loss+Losstmp.abs().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 600 == 0:\n",
        "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
        "                    % (epoch + 1, num_epochs, i + 1,\n",
        "                       len(train_set) // batch_size, loss.data.item()))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "for images, labels in test_loader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    testloss = criterion(outputs, labels)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Accuracy for test images: % d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7di0o7nVof90",
        "outputId": "ef92b030-4340-4821-dc5a-cd1718bfac91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training started for Task 2: Activation + Weight Quantization\n",
            "Epoch: [ 1/ 10], Step: [ 600/ 1875], Loss: 0.7032\n",
            "Epoch: [ 1/ 10], Step: [ 1200/ 1875], Loss: 0.7046\n",
            "Epoch: [ 1/ 10], Step: [ 1800/ 1875], Loss: 0.7637\n",
            "Epoch: [ 2/ 10], Step: [ 600/ 1875], Loss: 0.4903\n",
            "Epoch: [ 2/ 10], Step: [ 1200/ 1875], Loss: 0.5681\n",
            "Epoch: [ 2/ 10], Step: [ 1800/ 1875], Loss: 0.6625\n",
            "Epoch: [ 3/ 10], Step: [ 600/ 1875], Loss: 0.5949\n",
            "Epoch: [ 3/ 10], Step: [ 1200/ 1875], Loss: 0.6217\n",
            "Epoch: [ 3/ 10], Step: [ 1800/ 1875], Loss: 0.6995\n",
            "Epoch: [ 4/ 10], Step: [ 600/ 1875], Loss: 0.6880\n",
            "Epoch: [ 4/ 10], Step: [ 1200/ 1875], Loss: 0.7280\n",
            "Epoch: [ 4/ 10], Step: [ 1800/ 1875], Loss: 0.6007\n",
            "Epoch: [ 5/ 10], Step: [ 600/ 1875], Loss: 0.7549\n",
            "Epoch: [ 5/ 10], Step: [ 1200/ 1875], Loss: 0.8557\n",
            "Epoch: [ 5/ 10], Step: [ 1800/ 1875], Loss: 0.9410\n",
            "Epoch: [ 6/ 10], Step: [ 600/ 1875], Loss: 0.7295\n",
            "Epoch: [ 6/ 10], Step: [ 1200/ 1875], Loss: 0.8787\n",
            "Epoch: [ 6/ 10], Step: [ 1800/ 1875], Loss: 0.8188\n",
            "Epoch: [ 7/ 10], Step: [ 600/ 1875], Loss: 0.9400\n",
            "Epoch: [ 7/ 10], Step: [ 1200/ 1875], Loss: 0.8617\n",
            "Epoch: [ 7/ 10], Step: [ 1800/ 1875], Loss: 0.7874\n",
            "Epoch: [ 8/ 10], Step: [ 600/ 1875], Loss: 0.9506\n",
            "Epoch: [ 8/ 10], Step: [ 1200/ 1875], Loss: 0.8777\n",
            "Epoch: [ 8/ 10], Step: [ 1800/ 1875], Loss: 1.0350\n",
            "Epoch: [ 9/ 10], Step: [ 600/ 1875], Loss: 0.9746\n",
            "Epoch: [ 9/ 10], Step: [ 1200/ 1875], Loss: 0.8967\n",
            "Epoch: [ 9/ 10], Step: [ 1800/ 1875], Loss: 0.9474\n",
            "Epoch: [ 10/ 10], Step: [ 600/ 1875], Loss: 1.0259\n",
            "Epoch: [ 10/ 10], Step: [ 1200/ 1875], Loss: 1.0191\n",
            "Epoch: [ 10/ 10], Step: [ 1800/ 1875], Loss: 1.2285\n",
            "Accuracy for test images:  74 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Pruning 1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.cuda()\n",
        "prune_amounts = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "print(\"Filter-wise pruning based on L1 norm\")\n",
        "for amount in prune_amounts:\n",
        "\n",
        "    pruned_model = deepcopy(load_model)\n",
        "\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            # prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(pruned_model, f\"Filter-wise pruning based on L1 norm, amount={amount}\")"
      ],
      "metadata": {
        "id": "FIFoacIQ06AD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ad0e14-e153-4b79-daf1-654c673093fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter-wise pruning based on L1 norm\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.05\n",
            "Accuracy on the 10000 test images:  90.25 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.05\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 135 | Total params: 144 | Sparsity: 6.25%\n",
            "Layer: conv2.weight | Non-zero params: 4320 | Total params: 4608 | Sparsity: 6.25%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20135 non-zero params in total\n",
            "\n",
            "Approximate model size: 78.65 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.1\n",
            "Accuracy on the 10000 test images:  90.19 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.1\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 126 | Total params: 144 | Sparsity: 12.50%\n",
            "Layer: conv2.weight | Non-zero params: 4176 | Total params: 4608 | Sparsity: 9.38%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 19982 non-zero params in total\n",
            "\n",
            "Approximate model size: 78.05 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.2\n",
            "Accuracy on the 10000 test images:  88.15 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.2\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 117 | Total params: 144 | Sparsity: 18.75%\n",
            "Layer: conv2.weight | Non-zero params: 3744 | Total params: 4608 | Sparsity: 18.75%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 19541 non-zero params in total\n",
            "\n",
            "Approximate model size: 76.33 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.3\n",
            "Accuracy on the 10000 test images:  80.66 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.3\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 99 | Total params: 144 | Sparsity: 31.25%\n",
            "Layer: conv2.weight | Non-zero params: 3168 | Total params: 4608 | Sparsity: 31.25%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18947 non-zero params in total\n",
            "\n",
            "Approximate model size: 74.01 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.4\n",
            "Accuracy on the 10000 test images:  77.03 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.4\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 90 | Total params: 144 | Sparsity: 37.50%\n",
            "Layer: conv2.weight | Non-zero params: 2736 | Total params: 4608 | Sparsity: 40.62%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18506 non-zero params in total\n",
            "\n",
            "Approximate model size: 72.29 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Filter-wise pruning based on L1 norm, amount=0.5\n",
            "Accuracy on the 10000 test images:  63.00 %\n",
            "\n",
            "Size of Filter-wise pruning based on L1 norm, amount=0.5\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 72 | Total params: 144 | Sparsity: 50.00%\n",
            "Layer: conv2.weight | Non-zero params: 2304 | Total params: 4608 | Sparsity: 50.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18056 non-zero params in total\n",
            "\n",
            "Approximate model size: 70.53 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Pruning 2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.cuda()\n",
        "prune_amounts = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "print(\"Channel-wise pruning based on L1 and L2 norm\")\n",
        "for amount in prune_amounts:\n",
        "\n",
        "    pruned_model = deepcopy(load_model)\n",
        "\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=1)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(pruned_model, f\"Channel-wise pruning based on L1 and L2 norm, amount={amount}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS0YJc47bKI6",
        "outputId": "c9d78f27-a4d9-4258-8dde-428ead88b90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel-wise pruning based on L1 and L2 norm\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.05\n",
            "Accuracy on the 10000 test images:  90.20 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.05\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4032 | Total params: 4608 | Sparsity: 12.50%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 19856 non-zero params in total\n",
            "\n",
            "Approximate model size: 77.56 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.1\n",
            "Accuracy on the 10000 test images:  90.09 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.1\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 3744 | Total params: 4608 | Sparsity: 18.75%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 19568 non-zero params in total\n",
            "\n",
            "Approximate model size: 76.44 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.2\n",
            "Accuracy on the 10000 test images:  89.81 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.2\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 2880 | Total params: 4608 | Sparsity: 37.50%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18704 non-zero params in total\n",
            "\n",
            "Approximate model size: 73.06 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.3\n",
            "Accuracy on the 10000 test images:  88.86 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.3\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 2304 | Total params: 4608 | Sparsity: 50.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18128 non-zero params in total\n",
            "\n",
            "Approximate model size: 70.81 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.4\n",
            "Accuracy on the 10000 test images:  86.83 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.4\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 1728 | Total params: 4608 | Sparsity: 62.50%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 17552 non-zero params in total\n",
            "\n",
            "Approximate model size: 68.56 KB\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "Evaluating Channel-wise pruning based on L1 and L2 norm, amount=0.5\n",
            "Accuracy on the 10000 test images:  83.90 %\n",
            "\n",
            "Size of Channel-wise pruning based on L1 and L2 norm, amount=0.5\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 1152 | Total params: 4608 | Sparsity: 75.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 16976 non-zero params in total\n",
            "\n",
            "Approximate model size: 66.31 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Optimized pruning\n",
        "prune_amounts = [0.1, 0.2, 0.22, 0.3]\n",
        "evaluate_model(load_model, \"Original Model\")\n",
        "\n",
        "print(\"Combine both Channel and filter -wise pruning\")\n",
        "for amount in prune_amounts:\n",
        "\n",
        "    pruned_model = deepcopy(load_model)\n",
        "\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=1)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(pruned_model, \"Pruned Model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4jaFeTHrA6E",
        "outputId": "6aeec4d7-9c71-4671-fa44-1f98211873b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "Size of Original Model\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 144 | Total params: 144 | Sparsity: 0.00%\n",
            "Layer: conv2.weight | Non-zero params: 4608 | Total params: 4608 | Sparsity: 0.00%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 20432 non-zero params in total\n",
            "\n",
            "Approximate model size: 79.81 KB\n",
            "-------------------------------------\n",
            "Combine both Channel and filter -wise pruning\n",
            "With prune amount of 0.1 Accuracy for test images:  90 %\n",
            "-------------------------------------\n",
            "Size of Pruned Model\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 126 | Total params: 144 | Sparsity: 12.50%\n",
            "Layer: conv2.weight | Non-zero params: 3393 | Total params: 4608 | Sparsity: 26.37%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 19199 non-zero params in total\n",
            "\n",
            "Approximate model size: 75.00 KB\n",
            "-------------------------------------\n",
            "With prune amount of 0.2 Accuracy for test images:  88 %\n",
            "-------------------------------------\n",
            "Size of Pruned Model\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 117 | Total params: 144 | Sparsity: 18.75%\n",
            "Layer: conv2.weight | Non-zero params: 2340 | Total params: 4608 | Sparsity: 49.22%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 18137 non-zero params in total\n",
            "\n",
            "Approximate model size: 70.85 KB\n",
            "-------------------------------------\n",
            "With prune amount of 0.22 Accuracy for test images:  80 %\n",
            "-------------------------------------\n",
            "Size of Pruned Model\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 108 | Total params: 144 | Sparsity: 25.00%\n",
            "Layer: conv2.weight | Non-zero params: 2025 | Total params: 4608 | Sparsity: 56.05%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 17813 non-zero params in total\n",
            "\n",
            "Approximate model size: 69.58 KB\n",
            "-------------------------------------\n",
            "With prune amount of 0.3 Accuracy for test images:  79 %\n",
            "-------------------------------------\n",
            "Size of Pruned Model\n",
            "Total parameters: 20432\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 99 | Total params: 144 | Sparsity: 31.25%\n",
            "Layer: conv2.weight | Non-zero params: 1584 | Total params: 4608 | Sparsity: 65.62%\n",
            "Layer: lin2.weight | Non-zero params: 15680 | Total params: 15680 | Sparsity: 0.00%\n",
            "Model has 17363 non-zero params in total\n",
            "\n",
            "Approximate model size: 67.82 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Combine both quantization and pruning to achieve an 80% accurate model\n",
        "### that is as small as possible\n",
        "\n",
        "quantized_model = deepcopy(load_model)\n",
        "new_sd = load_model.state_dict()\n",
        "\n",
        "# Quantize the model\n",
        "n_bits = 4\n",
        "print(f'Perform per-layer quantization with bit width of {n_bits} bits')\n",
        "for name,_ in load_model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    new_sd[name] = squant(torch.max(torch.abs(sd[name])), n_bits, sd[name])\n",
        "\n",
        "quantized_model.load_state_dict(new_sd)\n",
        "\n",
        "prune_amounts = [0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# REALLY aggressive pruning\n",
        "for amount in prune_amounts:\n",
        "    print(f'Prune the model with amount = {amount * 100} % :')\n",
        "    pruned_model = deepcopy(quantized_model)\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=1)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(pruned_model, \"80% accurate Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMjELek5fSW7",
        "outputId": "3e3fa5c8-c342-496b-bca7-699c68506892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perform per-layer quantization with bit width of 4 bits\n",
            "Prune the model with amount = 30.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  82.55 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 114 | Total params: 144 | Sparsity: 20.83%\n",
            "Layer: conv2.weight | Non-zero params: 1667 | Total params: 4608 | Sparsity: 63.82%\n",
            "Layer: lin2.weight | Non-zero params: 8782 | Total params: 15680 | Sparsity: 43.99%\n",
            "Model has 10563 non-zero params in total\n",
            "\n",
            "Approximate model size: 41.26 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 40.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  83.16 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 114 | Total params: 144 | Sparsity: 20.83%\n",
            "Layer: conv2.weight | Non-zero params: 1268 | Total params: 4608 | Sparsity: 72.48%\n",
            "Layer: lin2.weight | Non-zero params: 8782 | Total params: 15680 | Sparsity: 43.99%\n",
            "Model has 10164 non-zero params in total\n",
            "\n",
            "Approximate model size: 39.70 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 50.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  80.71 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 114 | Total params: 144 | Sparsity: 20.83%\n",
            "Layer: conv2.weight | Non-zero params: 864 | Total params: 4608 | Sparsity: 81.25%\n",
            "Layer: lin2.weight | Non-zero params: 8782 | Total params: 15680 | Sparsity: 43.99%\n",
            "Model has 9760 non-zero params in total\n",
            "\n",
            "Approximate model size: 38.12 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 60.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  10.00 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 0 | Total params: 144 | Sparsity: 100.00%\n",
            "Layer: conv2.weight | Non-zero params: 436 | Total params: 4608 | Sparsity: 90.54%\n",
            "Layer: lin2.weight | Non-zero params: 8782 | Total params: 15680 | Sparsity: 43.99%\n",
            "Model has 9218 non-zero params in total\n",
            "\n",
            "Approximate model size: 36.01 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Combine both quantization and pruning to achieve an 85% accurate model\n",
        "### that is as small as possible\n",
        "\n",
        "quantized_model = deepcopy(load_model)\n",
        "new_sd = load_model.state_dict()\n",
        "\n",
        "# Quantize the model\n",
        "n_bits = 4\n",
        "print(f'Perform per-layer quantization with bit width of {n_bits} bits')\n",
        "for name,_ in load_model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    new_sd[name] = squant(torch.max(torch.abs(sd[name])), n_bits, sd[name])\n",
        "\n",
        "quantized_model.load_state_dict(new_sd)\n",
        "\n",
        "prune_amounts = [0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36]\n",
        "\n",
        "# Run only Channel-wise pruning so we don't completely destroy accuracy\n",
        "for amount in prune_amounts:\n",
        "    print(f'Prune the model with amount = {amount * 100} % :')\n",
        "    pruned_model = deepcopy(quantized_model)\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=1)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(pruned_model, \"85% accurate Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZDC4wmtPxhX",
        "outputId": "421c1e37-9826-4749-8516-16a6b3baab92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perform per-layer quantization with bit width of 4 bits\n",
            "Prune the model with amount = 30.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  85.71 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1663 | Total params: 4608 | Sparsity: 63.91%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10609 non-zero params in total\n",
            "\n",
            "Approximate model size: 41.44 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 31.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  85.71 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1663 | Total params: 4608 | Sparsity: 63.91%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10609 non-zero params in total\n",
            "\n",
            "Approximate model size: 41.44 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 32.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  82.97 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1460 | Total params: 4608 | Sparsity: 68.32%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10406 non-zero params in total\n",
            "\n",
            "Approximate model size: 40.65 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 33.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  82.97 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1460 | Total params: 4608 | Sparsity: 68.32%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10406 non-zero params in total\n",
            "\n",
            "Approximate model size: 40.65 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 34.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  82.97 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1460 | Total params: 4608 | Sparsity: 68.32%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10406 non-zero params in total\n",
            "\n",
            "Approximate model size: 40.65 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 35.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  83.30 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1266 | Total params: 4608 | Sparsity: 72.53%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10212 non-zero params in total\n",
            "\n",
            "Approximate model size: 39.89 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 36.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 85% accurate Model\n",
            "Accuracy on the 10000 test images:  83.30 %\n",
            "\n",
            "Size of 85% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 110 | Total params: 144 | Sparsity: 23.61%\n",
            "Layer: conv2.weight | Non-zero params: 1266 | Total params: 4608 | Sparsity: 72.53%\n",
            "Layer: lin2.weight | Non-zero params: 8836 | Total params: 15680 | Sparsity: 43.65%\n",
            "Model has 10212 non-zero params in total\n",
            "\n",
            "Approximate model size: 39.89 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Combine both quantization and pruning to achieve a 90% accurate model\n",
        "\n",
        "optimized_model = deepcopy(load_model)\n",
        "new_sd = load_model.state_dict()\n",
        "\n",
        "# Quantize the model\n",
        "n_bits = 8\n",
        "print(f'Perform per-layer quantization with bit width of {n_bits} bits')\n",
        "for name,_ in load_model.named_parameters():\n",
        "    # print('quantizing ',name)\n",
        "    new_sd[name] = squant(torch.max(torch.abs(sd[name])), n_bits, sd[name])\n",
        "\n",
        "optimized_model.load_state_dict(new_sd)\n",
        "\n",
        "prune_amounts = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
        "# Run only Channel-wise pruning\n",
        "for amount in prune_amounts:\n",
        "    print(f'Prune the model with amount = {amount * 100} % :')\n",
        "    for name, module in optimized_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=1)\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    evaluate_model(optimized_model, \"80% accurate Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF-ZlNuJd00M",
        "outputId": "5b40451e-1507-42dd-b637-45d2c03d8668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perform per-layer quantization with bit width of 8 bits\n",
            "Prune the model with amount = 1.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  90.24 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 141 | Total params: 144 | Sparsity: 2.08%\n",
            "Layer: conv2.weight | Non-zero params: 4499 | Total params: 4608 | Sparsity: 2.37%\n",
            "Layer: lin2.weight | Non-zero params: 15127 | Total params: 15680 | Sparsity: 3.53%\n",
            "Model has 19767 non-zero params in total\n",
            "\n",
            "Approximate model size: 77.21 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 5.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  90.27 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 133 | Total params: 144 | Sparsity: 7.64%\n",
            "Layer: conv2.weight | Non-zero params: 3712 | Total params: 4608 | Sparsity: 19.44%\n",
            "Layer: lin2.weight | Non-zero params: 15127 | Total params: 15680 | Sparsity: 3.53%\n",
            "Model has 18972 non-zero params in total\n",
            "\n",
            "Approximate model size: 74.11 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 10.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  90.07 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 124 | Total params: 144 | Sparsity: 13.89%\n",
            "Layer: conv2.weight | Non-zero params: 3339 | Total params: 4608 | Sparsity: 27.54%\n",
            "Layer: lin2.weight | Non-zero params: 15127 | Total params: 15680 | Sparsity: 3.53%\n",
            "Model has 18590 non-zero params in total\n",
            "\n",
            "Approximate model size: 72.62 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 15.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  89.81 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 124 | Total params: 144 | Sparsity: 13.89%\n",
            "Layer: conv2.weight | Non-zero params: 2872 | Total params: 4608 | Sparsity: 37.67%\n",
            "Layer: lin2.weight | Non-zero params: 15127 | Total params: 15680 | Sparsity: 3.53%\n",
            "Model has 18123 non-zero params in total\n",
            "\n",
            "Approximate model size: 70.79 KB\n",
            "-------------------------------------\n",
            "Prune the model with amount = 20.0 % :\n",
            "-------------------------------------\n",
            "Evaluating 80% accurate Model\n",
            "Accuracy on the 10000 test images:  87.89 %\n",
            "\n",
            "Size of 80% accurate Model\n",
            "Total parameters: 20432\n",
            "\n",
            "Parameters per layer:\n",
            "Layer: conv1.weight | Non-zero params: 115 | Total params: 144 | Sparsity: 20.14%\n",
            "Layer: conv2.weight | Non-zero params: 2312 | Total params: 4608 | Sparsity: 49.83%\n",
            "Layer: lin2.weight | Non-zero params: 15127 | Total params: 15680 | Sparsity: 3.53%\n",
            "Model has 17554 non-zero params in total\n",
            "\n",
            "Approximate model size: 68.57 KB\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MyConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act1  = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.act2  = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.lin2  = nn.Linear(7*7*32, 10, bias=False)\n",
        "\n",
        "    def asquant(self, max_value, min_value, num_bits, x):\n",
        "        # Uniform Asymetric quantizer that\n",
        "        # quantize x into min_value + sf * qx\n",
        "        # sf: scaling factor\n",
        "        # qx: integer in range [0,2^bits-1]\n",
        "        # note: 2^bits different values can be represented, bits >= 1\n",
        "\n",
        "        sf = (max_value - min_value) / (2**num_bits - 1)\n",
        "        qx = stochastic_round((x-min_value)/sf)\n",
        "        qx = torch.clip(qx,min=0,max=2**(num_bits) - 1)\n",
        "        dqx = min_value + qx * sf\n",
        "        return dqx\n",
        "\n",
        "\n",
        "    def save_weights(self, path):\n",
        "        sd = self.state_dict()\n",
        "        quant_dict = self.state_dict()\n",
        "        for name,_ in self.named_parameters():\n",
        "            # print('quantizing ',name)\n",
        "            quant_dict[name] = self.asquant(torch.max(sd[name]), torch.min(sd[name]),\n",
        "                                                      n_bits, sd[name])\n",
        "        torch.save(quant_dict, path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MyConvNet(args)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion=criterion.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 5e-4, momentum=0.9)"
      ],
      "metadata": {
        "id": "xzs-vpl8iPhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}